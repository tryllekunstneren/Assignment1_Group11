{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link to github repo\n",
    "\n",
    "https://github.com/tryllekunstneren/Assignment1_Group11/blob/main/Assignment1.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Computational Social Science \n",
    "\n",
    "##### Christian Warburg and Sofus Carstens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Web-scraping\n",
    "Week 1, ex 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise: Web-scraping the list of participants to the International Conference in Computational Social Science**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Inspect the HTML of the page and use web-scraping to get the names of all researchers that contributed to the conference in 2023. The goal is the following: (i) get as many names as possible including: keynote speakers, chairs, authors of parallel talks and authors of posters; (ii) ensure that the collected names are complete and accuarate as reported in the website (e.g. both first name and family name); (iii) ensure that no name is repeated multiple times with slightly different spelling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. Some instructions for success: \n",
    ">    * First, inspect the page through your web browser to identify the elements of the page that you want to collect. Ensure you understand the hierarchical structure of the page, and where the elements you are interested in are located within this nested structure.   \n",
    ">    * Use the [BeautifulSoup Python package](https://pypi.org/project/beautifulsoup4/) to navigate through the hierarchy and extract the elements you need from the page. \n",
    ">    * You can use the [find_all](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all) method to find elements that match specific filters. Check the [documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) of the library for detailed explanations on how to set filters.  \n",
    ">    * Parse the strings to ensure that you retrieve \"clean\" author names (e.g. remove commas, or other unwanted charachters)\n",
    ">    * The overall idea is to adapt the procedure I have used [here](https://nbviewer.org/github/lalessan/comsocsci2023/blob/master/additional_notebooks/ScreenScraping.ipynb) for the specific page you are scraping. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3. Create the set of unique researchers that joined the conference and *store it into a file*.\n",
    ">     * *Important:* If you notice any issue with the list of names you have collected (e.g. duplicate/incorrect names), come up with a strategy to clean your list as much as possible. \n",
    ">\n",
    "##### ANSWER\n",
    "\n",
    "To take care of duplicates, we converted the list of names into a set, and then back to a list. For typos in names, we used fuzzy matching with Python’s difflib. By comparing names with a similarity threshold (0.9), similar entries (e.g., names with typos) were grouped together, and a single representative was chosen for each group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 4. *Optional:* For a more complete represenation of the field, include in your list: (i) the names of researchers from the programme committee of the conference, that can be found at [this link](https://ic2s2-2023.org/program_committee); (ii) the organizers of tutorials, that can be found at [this link](https://ic2s2-2023.org/tutorials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 5. How many unique researchers do you get?\n",
    "\n",
    "#### Answer\n",
    "We got 1524 unique authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 6. Explain the process you followed to web-scrape the page. Which choices did you make to accurately retreive as many names as possible? Which strategies did you use to assess the quality of your final list? Explain your reasoning and your choices __(answer in max 150 words)__.\n",
    ">\n",
    "##### Answer:\n",
    "\n",
    "We started by sending a GET request to the target URL and parsing its HTML content using BeautifulSoup. Recognizing that names were enclosed within `<i>` tags, we extracted their text and replaced newline characters for cleaner data. To address cases where multiple names appeared in a single tag, we split the text using “, ” as a delimiter. Converting the resulting list to a set removed duplicates, ensuring only unique names remained. Finally, we created a DataFrame for further validation. We assessed the quality of our final list by verifying the unique count of names and manually inspecting the formatting to confirm consistency and accuracy in retrieval. This systematic approach enabled us to maximize the extraction of correctly formatted names while minimizing redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique names in the list after typo correction: 1499\n",
      "Number of names removed as typos: 35\n",
      "Removed names: ['Xindi Wang', 'Luca Verginer', 'Luca Verginer', 'Duncan J. Watts', 'Duncan J. Watts', 'Duncan J. Watts', 'David M Rothschild', 'Anne C. Kroon', 'Michele Tizzoni', 'Michele Tizzoni', 'Michele Tizzoni', 'Fabio Carrella', 'Alessandro Flammini', 'Kathyrn R Fair', 'Woo-sung Jung', 'Lisette Espin Noboa', 'Nicholas A Christakis', 'Pantelis P. Analytis', 'Pantelis P Analytis', 'Sonja M Schmer Galunder', 'Bedoor AlShebli', 'Bedoor AlShebli', 'Bedoor AlShebli', 'Mariano Gaston Beiro', 'Diogo Pachecho', 'Marton Karsai', 'Marton Karsai', 'José Javier Ramasco', 'Federico Barrera-Lemarchand', 'Scott A. Hale', 'Scott A. Hale', 'Scott A. Hale', 'Marcos A. Oliveira', 'Matthew R DeVerna', 'Ana Maria Jaramillo']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import difflib\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "link = 'https://ic2s2-2023.org/program'\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "r = requests.get(link)\n",
    "\n",
    "# Parse the content of the webpage with BeautifulSoup using the html.parser\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "# Find all <i> tags in the HTML (where names are located)\n",
    "names = soup.find_all('i')\n",
    "\n",
    "# Extract text from each <i> tag and clean up whitespace/newlines\n",
    "raw_names = [tag.get_text(strip=True).replace(\"\\n\", \" \") for tag in names]\n",
    "\n",
    "# Use regex to split on comma followed by any whitespace, then remove quotes\n",
    "names_list = [re.split(r',\\s*', name) for name in raw_names]\n",
    "names_flat = [name.strip('\"') for sublist in names_list for name in sublist]\n",
    "\n",
    "# Count the frequency of each name (optional)\n",
    "name_counts = Counter(names_flat)\n",
    "\n",
    "# Function to group similar names using fuzzy matching\n",
    "def group_similar_names(names, threshold=0.9):\n",
    "    names_copy = names.copy()\n",
    "    groups = []\n",
    "    while names_copy:\n",
    "        base = names_copy.pop(0)\n",
    "        group = [base]\n",
    "        to_remove = []\n",
    "        for other in names_copy:\n",
    "            similarity = difflib.SequenceMatcher(None, base.lower(), other.lower()).ratio()\n",
    "            if similarity >= threshold:\n",
    "                group.append(other)\n",
    "                to_remove.append(other)\n",
    "        for name in to_remove:\n",
    "            names_copy.remove(name)\n",
    "        groups.append(group)\n",
    "    return groups\n",
    "\n",
    "# Group the names with a similarity threshold of 0.9\n",
    "groups = group_similar_names(names_flat, threshold=0.9)\n",
    "\n",
    "# Function to choose the best candidate from a group\n",
    "def choose_best_candidate(group):\n",
    "    return min(group, key=len)\n",
    "\n",
    "final_names = []\n",
    "removed_names = []\n",
    "\n",
    "for group in groups:\n",
    "    candidate = choose_best_candidate(group)\n",
    "    final_names.append(candidate)\n",
    "    removed = [name for name in group if name != candidate]\n",
    "    removed_names.extend(removed)\n",
    "\n",
    "# Create a DataFrame and export to CSV\n",
    "df = pd.DataFrame(final_names, columns=['Author'])\n",
    "df.to_csv('authors.csv', index=False)\n",
    "\n",
    "# Print the results\n",
    "print(f'Unique names in the list after typo correction: {len(df)}')\n",
    "print(f'Number of names removed as typos: {len(removed_names)}')\n",
    "print('Removed names:', removed_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Ready Made vs Custom Made Data\n",
    "Week 2, ex 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise: Ready made data vs Custom made data** In this exercise, I want to make sure you have understood they key points of my lecture and the reading. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. What are pros and cons of the custom-made data used in Centola's experiment (the first study presented in the lecture) and the ready-made data used in Nicolaides's study (the second study presented in the lecture)? You can support your arguments based on the content of the lecture and the information you read in Chapter 2.3 of the book __(answer in max 150 words)__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "Centola’s experiment used custom-made data, allowing for controlled conditions, precise measurement of social influence, and elimination of confounding variables. Researchers could manipulate network structures and directly observe behavioral changes, ensuring strong internal validity. However, the artificial setting may reduce external validity, as participants might behave differently in real-world contexts. Additionally, sample sizes are often smaller due to resource constraints. Nicolaides’s study used ready-made data from real-world sources, providing large-scale insights into disease transmission and high external validity. However, this data contains biases, lacks control over confounding factors, and may have measurement inaccuracies, such as missing data or inconsistencies in self-reported behaviors. Since observational data is not designed for experimental purposes, causality is harder to establish, requiring careful statistical modeling to infer relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. How do you think these differences can influence the interpretation of the results in each study? __(answer in max 150 words)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "Centola’s controlled experiment ensures causality by isolating specific variables, making it easier to identify the mechanisms driving social contagion. However, the findings may not fully capture complex social behaviors outside the lab, limiting their generalizability. The artificial setting may also fail to reflect spontaneous, large-scale diffusion processes. In contrast, Nicolaides’s study reflects real-world patterns of human mobility and interactions, offering valuable insights into disease spread. Yet, the reliance on observational data means that multiple external factors, such as policy interventions or demographic differences, could influence the results. While Nicolaides’s study provides practical, large-scale implications, it requires careful interpretation to avoid confounding correlations with causation. These methodological differences shape how confidently each study’s results can be applied to broader social contexts, particularly in policymaking and behavioral interventions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Gathering Research Articles using the OpenAlex API\n",
    "Week 3, ex 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise : Collecting Research Articles from IC2S2 Authors**\n",
    ">\n",
    ">In this exercise, we'll leverage the OpenAlex API to gather information on research articles authored by participants of the IC2S2 2024 (NOT 2023) conference, referred to as *IC2S2 authors*. **Before you start, please ensure you read through the entire exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Steps:**\n",
    ">  \n",
    "> 1. **Retrieve Data:** Starting with the *authors* you identified in Week 2, Exercise 2, use the OpenAlex API [works endpoint](https://docs.openalex.org/api-entities/works) to fetch the research articles they have authored. For each article, retrieve the following details:\n",
    ">    - _id_: The unique OpenAlex ID for the work.\n",
    ">    - _publication_year_: The year the work was published.\n",
    ">    - _cited_by_count_: The number of times the work has been cited by other works.\n",
    ">    - _author_ids_: The OpenAlex IDs for the authors of the work.\n",
    ">    - _title_: The title of the work.\n",
    ">    - _abstract_inverted_index_: The abstract of the work, formatted as an inverted index.\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">     **Important Note on Paging:** By default, the OpenAlex API limits responses to 25 works per request. For more efficient data retrieval, I suggest to adjust this limit to 200 works per request. Even with this adjustment, you will need to implement pagination to access all available works for a given query. This ensures you can systematically retrieve the complete set of works beyond the initial 200. Find guidance on implementing pagination [here](https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/paging#cursor-paging)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. **Data Storage:** Organize the retrieved information into two Pandas DataFrames and save them to two files in a suitable format:\n",
    ">    - The *IC2S2 papers* dataset should include: *id, publication\\_year, cited\\_by\\_count, author\\_ids*.\n",
    ">    - The *IC2S2 abstracts* dataset should include: *id, title, abstract\\_inverted\\_index*.\n",
    ">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Filters:**\n",
    "> To ensure the data we collect is relevant and manageable, apply the following filters:\n",
    "> \n",
    ">    - Only include *IC2S2 authors* with a total work count between 5 and 5,000.\n",
    ">    - Retrieve only works that have received more than 10 citations.\n",
    ">    - Limit to works authored by fewer than 10 individuals.\n",
    ">    - Include only works relevant to Computational Social Science (focusing on: Sociology OR Psychology OR Economics OR Political Science) AND intersecting with a quantitative discipline (Mathematics OR Physics OR Computer Science), as defined by their [Concepts](https://docs.openalex.org/api-entities/works/work-object#concepts). *Note*: here we only consider Concepts at *level=0* (the most coarse definition of concepts). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Efficiency Tips:**\n",
    "> Writing efficient code in this exercise is **crucial**. To speed up your process:\n",
    "> - **Apply filters directly in your request:** When possible, use the [filter parameter](https://docs.openalex.org/api-entities/works/filter-works) of the *works* endpoint to apply the filters above directly in your API request, ensuring only relevant data is returned. Learn about combining multiple filters [here](https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/filter-entity-lists).  \n",
    "> - **Bulk requests:** Instead of sending one request for each author, you can use the [filter parameter](https://docs.openalex.org/api-entities/works/filter-works) to query works by multiple authors in a single request. *Note: My testing suggests that can only include up to 25 authors per request.*\n",
    "> - **Use multiprocessing:** Implement multiprocessing to handle multiple requests simultaneously. I highly recommmend [Joblib’s Parallel](https://joblib.readthedocs.io/en/stable/) function for that, and [tqdm](https://tqdm.github.io/) can help monitor progress of your jobs. Remember to stay within [the rate limit](https://docs.openalex.org/how-to-use-the-api/rate-limits-and-authentication) of 10 requests per second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Data Overview and Reflection questions:** Answer the following questions: \n",
    "> - **Dataset summary.** How many works are listed in your *IC2S2 papers* dataframe? How many unique researchers have co-authored these works? \n",
    "> - **Efficiency in code.** Describe the strategies you implemented to make your code more efficient. How did your approach affect your code's execution time? __(answer in max 150 words)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - **Filtering Criteria and Dataset Relevance** Reflect on the rationale behind setting specific thresholds for the total number of works by an author, the citation count, the number of authors per work, and the relevance of works to specific fields. How do these filtering criteria contribute to the relevance of the dataset you compiled? Do you believe any aspects of Computational Social Science research might be underrepresented or overrepresented as a result of these choices? __(answer in max 150 words)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Network of Computational Social Scientists\n",
    "Week 4, ex 1. Please use the final dataset you collected from both authors and co-authors (IC2S2 2024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   0%|          | 1/1499 [00:00<09:00,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Chair: Claudia Wagner\n",
      "Could not retrieve id for Jon Kleinberg\n",
      "Could not retrieve id for Xinyi Wang\n",
      "Could not retrieve id for Jonas L Juul\n",
      "Could not retrieve id for Chloe Ahn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   0%|          | 6/1499 [00:00<02:33,  9.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Giuseppe RussoCould not retrieve id for Giona Casiraghi\n",
      "\n",
      "Could not retrieve id for Manoel Horta Ribeiro\n",
      "Could not retrieve id for Almog Simchon\n",
      "Could not retrieve id for luca verginer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   1%|          | 11/1499 [00:02<04:46,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Matthew Edwards\n",
      "Could not retrieve id for Manuel Vimercati\n",
      "Could not retrieve id for Stephan Lewandowsky\n",
      "Could not retrieve id for Arianna Pera\n",
      "Could not retrieve id for Adam Sutton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   1%|          | 16/1499 [00:02<03:21,  7.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Mohammed Alsobay\n",
      "Could not retrieve id for Matteo Palmonari\n",
      "Could not retrieve id for David G. Rand\n",
      "Could not retrieve id for Duncan J Watts\n",
      "Could not retrieve id for Abdullah Almaatouq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   2%|▏         | 24/1499 [00:04<04:09,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Santo FortunatoCould not retrieve id for Francesco Rinaldi\n",
      "\n",
      "Could not retrieve id for Francesco Tudisco\n",
      "Could not retrieve id for Satyaki Sikdar\n",
      "Could not retrieve id for Sara Venturini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   2%|▏         | 29/1499 [00:04<02:59,  8.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Isabella Loaiza\n",
      "Could not retrieve id for Alex Pentland\n",
      "Could not retrieve id for Takahiro Yabe\n",
      "Could not retrieve id for Chair: Taha Yasseri\n",
      "Could not retrieve id for Silvia De Sojo Caso\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   2%|▏         | 31/1499 [00:05<03:53,  6.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Laura Alessandretti\n",
      "Could not retrieve id for Rubén Rodríguez Casañ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   2%|▏         | 33/1499 [00:05<04:58,  4.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Antonio Ariño Villarroya\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   2%|▏         | 34/1499 [00:06<05:14,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Emil Bakkensen Johansen\n",
      "Could not retrieve id for Mia Ann Jørgensen\n",
      "Could not retrieve id for Sunny Rai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   2%|▏         | 37/1499 [00:06<04:14,  5.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Ashley Francisco\n",
      "Could not retrieve id for Salvatore Giorgi\n",
      "Could not retrieve id for Brenda Curtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   3%|▎         | 40/1499 [00:06<03:39,  6.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Lyle Ungar\n",
      "Could not retrieve id for Sharath Chandra Guntuku\n",
      "Could not retrieve id for Allison Koenecke\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   3%|▎         | 43/1499 [00:07<03:21,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Eric Giannella\n",
      "Could not retrieve id for Sune Lehmann\n",
      "Could not retrieve id for Sharad Goel\n",
      "Could not retrieve id for Robb Willer\n",
      "Could not retrieve id for Mathias Wullum Nielsen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   3%|▎         | 52/1499 [00:07<02:00, 11.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Ziv Epstein\n",
      "Could not retrieve id for Hause Lin\n",
      "Could not retrieve id for Bramantyo Supriyatno\n",
      "Could not retrieve id for Levin Brinkmann\n",
      "Could not retrieve id for Iyad Rahwan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   4%|▎         | 54/1499 [00:07<02:09, 11.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Sandro Ferreira Sousa\n",
      "Could not retrieve id for Vincenzo Nicosia\n",
      "Could not retrieve id for Chair: David Garcia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   4%|▎         | 56/1499 [00:09<05:03,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Tiancheng Hu\n",
      "Could not retrieve id for Marilena Hohmann\n",
      "Could not retrieve id for Yara Kyrychenko\n",
      "Could not retrieve id for Michele Coscia\n",
      "Could not retrieve id for Sander van der Linden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   4%|▍         | 61/1499 [00:09<03:32,  6.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Jon Roozenbeek\n",
      "Could not retrieve id for Nikolaos Nakis\n",
      "Could not retrieve id for Louis Boucherie\n",
      "Could not retrieve id for Morten Mørup\n",
      "Could not retrieve id for Abdulkadir Celikkanat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   5%|▍         | 70/1499 [00:10<02:40,  8.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Jacopo D'Ignazi\n",
      "Could not retrieve id for Corrado Monti\n",
      "Could not retrieve id for Homa Hosseinmardi\n",
      "Could not retrieve id for Gianmarco De Francisci Morales\n",
      "Could not retrieve id for Michele Starnini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   5%|▌         | 75/1499 [00:10<02:12, 10.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Sam Wolken\n",
      "Could not retrieve id for Duncan Watts\n",
      "Could not retrieve id for David Rothschild\n",
      "Could not retrieve id for Li Zhang\n",
      "Could not retrieve id for Isabelle Lorge\n",
      "Could not retrieve id for Melanie Oyarzun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   5%|▌         | 77/1499 [00:11<03:50,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Cristian E Candia\n",
      "Could not retrieve id for Bernardo Garcia Bulle Bueno\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   5%|▌         | 79/1499 [00:12<04:42,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Xiaowen Dong\n",
      "Could not retrieve id for Janet B. Pierrehumbert\n",
      "Could not retrieve id for Chair: Milena Tsvetkova\n",
      "Could not retrieve id for Morgan Ryan Frank\n",
      "Could not retrieve id for Carlos Rodriguez-Sickert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   6%|▌         | 84/1499 [00:12<03:17,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Esteban Moro\n",
      "Could not retrieve id for Dingeman Jan Van der Laan\n",
      "Could not retrieve id for Edwin De Jonge\n",
      "Could not retrieve id for Marjolijn Das\n",
      "Could not retrieve id for Veniamin Veselovsky\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   6%|▌         | 89/1499 [00:12<02:38,  8.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Mark E Whiting\n",
      "Could not retrieve id for Robert West\n",
      "Could not retrieve id for Josep Perelló\n",
      "Could not retrieve id for Marc Sadurní Parera\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   6%|▌         | 93/1499 [00:13<02:24,  9.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Chair: Elisa Omodei\n",
      "Could not retrieve id for Jamell Dacon\n",
      "Could not retrieve id for Jiliang Tang\n",
      "Could not retrieve id for Minsu Park\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   6%|▋         | 97/1499 [00:13<02:15, 10.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Chao Yu\n",
      "Could not retrieve id for Michael Macy\n",
      "Could not retrieve id for Kat Albrecht\n",
      "Could not retrieve id for Nicola Fanton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   7%|▋         | 101/1499 [00:13<02:08, 10.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Michael Roth\n",
      "Could not retrieve id for Agnieszka Falenska\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   7%|▋         | 103/1499 [00:15<04:31,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Zeyneb Nahide Kaya\n",
      "Could not retrieve id for Miquel Montero\n",
      "Could not retrieve id for Chair: Sandra Gonzalez-Bailón\n",
      "Could not retrieve id for Alexandra Segerberg\n",
      "Could not retrieve id for Matteo Magnani\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   7%|▋         | 108/1499 [00:15<03:20,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Damian Trilling\n",
      "Could not retrieve id for Rupert Kiddle\n",
      "Could not retrieve id for Anne C Kroon\n",
      "Could not retrieve id for Zilin Lin\n",
      "Could not retrieve id for Roeland Dubèl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   8%|▊         | 113/1499 [00:16<03:14,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Mónika Simon\n",
      "Could not retrieve id for Susan Vermeer\n",
      "Could not retrieve id for Kasper Welbers\n",
      "Could not retrieve id for Mark Boukes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   8%|▊         | 117/1499 [00:16<02:51,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Sukankana Chakraborty\n",
      "Could not retrieve id for Leonardo Castro-Gonzalez\n",
      "Could not retrieve id for Helen Margetts\n",
      "Could not retrieve id for Jonathan Bright\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   8%|▊         | 121/1499 [00:16<02:34,  8.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Lisa Merten\n",
      "Could not retrieve id for Helena Sophia Rauxloh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   8%|▊         | 123/1499 [00:17<02:47,  8.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Judith Moeller\n",
      "Could not retrieve id for Heidi Schulze\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   8%|▊         | 125/1499 [00:17<03:05,  7.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Sebastian Stier\n",
      "Could not retrieve id for Ardon Z. Shorr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   8%|▊         | 127/1499 [00:17<03:14,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Dafna Yavetz\n",
      "Could not retrieve id for Chair: Leo Ferres\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   9%|▊         | 129/1499 [00:18<03:15,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Sanjay Kairam\n",
      "Could not retrieve id for Dylan Thurgood\n",
      "Could not retrieve id for Isabel Lerch\n",
      "Could not retrieve id for Simon Martin Breum\n",
      "Could not retrieve id for Bojan Kostic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   9%|▉         | 134/1499 [00:18<02:28,  9.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Zhuangyuan Fan\n",
      "Could not retrieve id for Becky P.Y. Loo\n",
      "Could not retrieve id for Michael Szell\n",
      "Could not retrieve id for Ting Lian\n",
      "Could not retrieve id for Feiyang Zhang\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   9%|▉         | 139/1499 [00:18<02:03, 11.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Bruno Lepri\n",
      "Could not retrieve id for Massimiliano Luca\n",
      "Could not retrieve id for Antonio Bucchiarone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   9%|▉         | 142/1499 [00:20<04:05,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Annapaola Marconi\n",
      "Could not retrieve id for Hugo Barbosa\n",
      "Could not retrieve id for Laura Maria Alessandretti\n",
      "Could not retrieve id for Simone Centellegher\n",
      "Could not retrieve id for Surendra Hazarie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:  10%|▉         | 147/1499 [00:20<03:07,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Ronaldo MenezesCould not retrieve id for Henrik Wolf\n",
      "\n",
      "Could not retrieve id for Gourab Ghoshal\n",
      "Could not retrieve id for Ane Rahbek Vierø\n",
      "Could not retrieve id for Adam Frank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:  10%|█         | 152/1499 [00:21<03:06,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Viktoria Spaiser\n",
      "Could not retrieve id for Kelton R Minor\n",
      "Could not retrieve id for Nicole Nisbett\n",
      "Could not retrieve id for Chair: Clara Vandeweerdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve id for Hannah-Marie Büttner\n",
      "Could not retrieve id for Hendrik Meyer\n",
      "Could not retrieve id for Patrick Zerrer\n",
      "Could not retrieve id for Max Falkenberg\n",
      "Could not retrieve id for Andrea Baronchelli\n",
      "Could not retrieve id for Francesco Lamperti\n",
      "Could not retrieve id for Roberto MaviliaCould not retrieve id for Giorgio Tripodi\n",
      "\n",
      "Could not retrieve id for Andrea mina\n",
      "Could not retrieve id for Francesca Chiaromonte\n",
      "Could not retrieve id for Kristoffer Lind Glavind\n",
      "Could not retrieve id for Aaron J. Schwartz\n",
      "Could not retrieve id for Christopher Danforth\n",
      "Could not retrieve id for Chair: Andreia Sofia Teixeira\n",
      "Could not retrieve id for Andreas Bjerre-Nielsen\n",
      "Could not retrieve id for Michele Tizzani\n",
      "Could not retrieve id for Daniela Paolotti\n",
      "Could not retrieve id for Pietro Coletti\n",
      "Could not retrieve id for Alessandro De Gaetano\n",
      "Could not retrieve id for Christopher I Jarvis\n",
      "Could not retrieve id for Amy Gimma\n",
      "Could not retrieve id for Kerry Wong\n",
      "Could not retrieve id for John W Edmunds\n",
      "Could not retrieve id for Philippe Beutels\n",
      "Could not retrieve id for Niel Hens\n",
      "Could not retrieve id for Julia Koltai\n",
      "Could not retrieve id for Anna Sara Ligeti\n",
      "Could not retrieve id for Marcin Waniek\n",
      "Could not retrieve id for Petter Holme\n",
      "Could not retrieve id for Katayoun Farrahi\n",
      "Could not retrieve id for Remi Emonet\n",
      "Could not retrieve id for Manuel Cebrian\n",
      "Could not retrieve id for Adam Stefkovics\n",
      "Could not retrieve id for Nick Obradovich\n",
      "Could not retrieve id for Fabrizio Lillo\n",
      "Could not retrieve id for Talal Rahwan\n",
      "Could not retrieve id for Daniela Perrotta\n",
      "Could not retrieve id for Nicolò Gozzi\n",
      "Could not retrieve id for Nicola Perra\n",
      "Could not retrieve id for Paolo Bajardi\n",
      "Could not retrieve id for Yuka Takedomi\n",
      "Could not retrieve id for Yuri Nakayama\n",
      "Could not retrieve id for Ryota Kobayashi\n",
      "Could not retrieve id for Towa Suda\n",
      "Could not retrieve id for Lorenzo Amir Nemati Fard\n",
      "Could not retrieve id for Takeaki Uno\n",
      "Could not retrieve id for Luis E C Rocha\n",
      "Could not retrieve id for Chair: Edward Lee\n",
      "Could not retrieve id for Masaru Kitsuregawa\n",
      "Could not retrieve id for Naoki Yoshinaga\n",
      "Could not retrieve id for Takako Hashimoto\n",
      "Could not retrieve id for Masashi Toyoda\n",
      "Could not retrieve id for Katie Spoon\n",
      "Could not retrieve id for Isabelle Langrock\n",
      "Could not retrieve id for Marcelo Silva\n",
      "Could not retrieve id for Kristina Gligoric\n",
      "Could not retrieve id for Jack LaViolette\n",
      "Could not retrieve id for Tiziano Piccardi\n",
      "Could not retrieve id for Jake M. Hofman\n",
      "Could not retrieve id for Renzhe Yu\n",
      "Could not retrieve id for Sky CH-Wang\n",
      "Could not retrieve id for Zhuojian Wei\n",
      "Could not retrieve id for Zhen Xu\n",
      "Could not retrieve id for Nalette Brodnax\n",
      "Could not retrieve id for Cesar A Hidalgo\n",
      "Could not retrieve id for Chair: Luca Rossi\n",
      "Could not retrieve id for Mariana Macedo\n",
      "Could not retrieve id for Pascal Van Hentenryck\n",
      "Could not retrieve id for Keyu Zhu\n",
      "Could not retrieve id for Juniper L Lovato\n",
      "Could not retrieve id for Nabeel Gillani\n",
      "Could not retrieve id for Laurent Hebert-Dufresne\n",
      "Could not retrieve id for Jonathan St-Onge\n",
      "Could not retrieve id for Randall Harp\n",
      "Could not retrieve id for Gabriela Salazar Lopez\n",
      "Could not retrieve id for Sean P. Rogers\n",
      "Could not retrieve id for Jeremiah Onaolapo\n",
      "Could not retrieve id for Ijaz Ul Haq\n",
      "Could not retrieve id for Guangnan Zhu\n",
      "Could not retrieve id for Kateryna Kasianenko\n",
      "Could not retrieve id for Shima Saniei\n",
      "Could not retrieve id for Uttama Barua\n",
      "Could not retrieve id for Jana Lasser\n",
      "Could not retrieve id for Segun Aroyehun\n",
      "Could not retrieve id for Fabio Carella\n",
      "Could not retrieve id for George Gyamfi\n",
      "Could not retrieve id for David Garcia\n",
      "Could not retrieve id for Ruth Elisabeth Appel\n",
      "Could not retrieve id for Jennifer Pan\n",
      "Could not retrieve id for Bao Tran Truong\n",
      "Could not retrieve id for Margaret Roberts\n",
      "Could not retrieve id for Xiaodan Lou\n",
      "Could not retrieve id for Jason C Coronel\n",
      "Could not retrieve id for Filippo Menczer\n",
      "Could not retrieve id for Chair: Jonas L Juul\n",
      "Could not retrieve id for Yuanmo He\n",
      "Could not retrieve id for Milena Tsvetkova\n",
      "Could not retrieve id for Kathyrn Fair\n",
      "Could not retrieve id for Shannon Poulsen\n",
      "Could not retrieve id for Omar Guerrero\n",
      "Could not retrieve id for Gerardo Iñiguez\n",
      "Could not retrieve id for Sara Heydari\n",
      "Could not retrieve id for János Kertész\n",
      "Could not retrieve id for Jari Saramäki\n",
      "Could not retrieve id for Johanna Einsiedler\n",
      "Could not retrieve id for David Dreyer Lassen\n",
      "Could not retrieve id for Nikolaj Arpe Harmon\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "from pyalex import Works, Authors, config\n",
    "\n",
    "# Set your email (to use the polite pool)\n",
    "pyalex_email = \"s225083@dtu.dk\"  \n",
    "config.email = pyalex_email\n",
    "\n",
    "# Increase retry settings for transient issues\n",
    "config.max_retries = 5\n",
    "config.retry_backoff_factor = 0.5\n",
    "config.retry_http_codes = [429, 500, 503]\n",
    "\n",
    "# --- Define Helper Functions ---\n",
    "\n",
    "def get_author_id(author_name):\n",
    "    \"\"\"\n",
    "    Retrieve the OpenAlex ID for an author given their name.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pager = Authors().filter(display_name=author_name).paginate(per_page=1)\n",
    "        for page in pager:\n",
    "            if page:\n",
    "                return page[0].get(\"id\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        #print(f\"Error fetching id for author {author_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_author_works(author_id, per_page=100):\n",
    "    \"\"\"\n",
    "    Retrieve works for a given author applying the following filters:\n",
    "      - More than 10 citations.\n",
    "      - Authored by fewer than 10 individuals.\n",
    "      - Concepts at level 0.\n",
    "      - Relevant to Computational Social Science (Sociology OR Psychology OR Economics OR Political Science)\n",
    "        AND intersecting with a quantitative discipline (Mathematics OR Physics OR Computer Science).\n",
    "    \"\"\"\n",
    "    # Define concept groups (replace placeholder IDs with actual OpenAlex concept IDs as needed)\n",
    "    concepts_group_a = \"C.SOC|C.PSY|C.ECO|C.POL\"\n",
    "    concepts_group_b = \"C.MAT|C.PHY|C.CS\"\n",
    "    \n",
    "    filter_string = (\n",
    "         f\"author.id:{author_id},\"\n",
    "         \"cited_by_count:>10,\"\n",
    "         \"authorships_count:<10,\"\n",
    "         \"concepts.level:0,\"\n",
    "         f\"concepts.id:({concepts_group_a}),\"\n",
    "         f\"concepts.id:({concepts_group_b})\"\n",
    "    )\n",
    "    \n",
    "    all_works = []\n",
    "    try:\n",
    "        pager = Works().filter(filter_string).paginate(per_page=per_page)\n",
    "        for page_num, page in enumerate(tqdm(pager, desc=f'Author {author_id}', leave=False), start=1):\n",
    "            all_works.extend(page)\n",
    "            print(f\"Author {author_id} - Page {page_num}: Retrieved {len(page)} works.\")\n",
    "            time.sleep(1)  # Pause to respect rate limits\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching works for author {author_id}: {e}\")\n",
    "    return all_works\n",
    "\n",
    "def fetch_author_works_by_name(author_name):\n",
    "    \"\"\"\n",
    "    Given an author name, retrieve their OpenAlex ID and fetch works using the above filters.\n",
    "    \"\"\"\n",
    "    #print(f\"\\nFetching works for author: {author_name}\")\n",
    "    author_id = get_author_id(author_name)\n",
    "    if not author_id:\n",
    "        print(f\"Could not retrieve id for {author_name}\")\n",
    "        return author_name, []\n",
    "    works = get_author_works(author_id)\n",
    "    time.sleep(1)\n",
    "    return author_name, works\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "def main():\n",
    "    # Load the CSV containing a list of author names (assumed to have a column \"name\")\n",
    "    authors_df = pd.read_csv('authors.csv')\n",
    "    author_names = authors_df[\"Author\"].tolist()\n",
    "    \n",
    "    author_works = {}\n",
    "\n",
    "    # Fetch works concurrently for each author using tqdm to monitor progress.\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = {executor.submit(fetch_author_works_by_name, name): name for name in author_names}\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing authors\"):\n",
    "            name = futures[future]\n",
    "            try:\n",
    "                author_name, works = future.result()\n",
    "                author_works[author_name] = works\n",
    "            except Exception as exc:\n",
    "                print(f\"Author {name} generated an exception: {exc}\")\n",
    "\n",
    "    # Aggregate and deduplicate works based on work ID.\n",
    "    all_works = []\n",
    "    for works in author_works.values():\n",
    "        all_works.extend(works)\n",
    "    works_dict = {work.get(\"id\"): work for work in all_works}\n",
    "    all_works = list(works_dict.values())\n",
    "    print(f\"Total works retrieved after deduplication: {len(all_works)}\")\n",
    "    \n",
    "    # --- Create DataFrames for the Two Datasets ---\n",
    "\n",
    "    def extract_paper_details(work):\n",
    "        \"\"\"\n",
    "        For the IC2S2 papers dataset: extract id, publication_year, cited_by_count, and author_ids.\n",
    "        \"\"\"\n",
    "        author_ids_list = []\n",
    "        if \"authorships\" in work:\n",
    "            author_ids_list = [auth.get(\"author\", {}).get(\"id\") for auth in work[\"authorships\"] if auth.get(\"author\", {}).get(\"id\")]\n",
    "        return {\n",
    "            \"id\": work.get(\"id\"),\n",
    "            \"publication_year\": work.get(\"publication_year\"),\n",
    "            \"cited_by_count\": work.get(\"cited_by_count\"),\n",
    "            \"author_ids\": author_ids_list,\n",
    "        }\n",
    "\n",
    "    def extract_abstract_details(work):\n",
    "        \"\"\"\n",
    "        For the IC2S2 abstracts dataset: extract id, title, and abstract_inverted_index.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"id\": work.get(\"id\"),\n",
    "            \"title\": work.get(\"display_name\", \"\"),\n",
    "            \"abstract_inverted_index\": work.get(\"abstract_inverted_index\"),\n",
    "        }\n",
    "\n",
    "    papers_data = [extract_paper_details(work) for work in all_works]\n",
    "    abstracts_data = [extract_abstract_details(work) for work in all_works]\n",
    "\n",
    "    papers_df = pd.DataFrame(papers_data)\n",
    "    abstracts_df = pd.DataFrame(abstracts_data)\n",
    "\n",
    "    # Save the DataFrames to CSV files.\n",
    "    papers_df.to_csv(\"IC2S2_papers.csv\", index=False)\n",
    "    abstracts_df.to_csv(\"IC2S2_abstracts.csv\", index=False)\n",
    "\n",
    "    print(\"IC2S2 papers and abstracts datasets have been saved.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise: Constructing the Computational Social Scientists Network**\n",
    ">\n",
    "> In this exercise, we will create a network of researchers in the field of Computational Social Science using the NetworkX library. In our network, nodes represent authors of academic papers, with a direct link from node _A_ to node _B_ indicating a joint paper written by both. The link's weight reflects the number of papers written by both _A_ and _B_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Part 1: Network Construction**\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. **Weighted Edgelist Creation:** Start with your dataframe of *papers*. Construct a _weighted edgelist_ where each list element is a tuple containing three elements: the _author ids_ of two collaborating authors and the total number of papers they've co-authored. Ensure each author pair is listed only once. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. **Graph Construction:**\n",
    ">    - Use NetworkX to create an undirected [``Graph``](https://networkx.org/documentation/stable/reference/classes/graph.html).\n",
    ">    - Employ the [`add_weighted_edges_from`](https://networkx.org/documentation/stable/reference/classes/generated/networkx.Graph.add_weighted_edges_from.html#networkx.Graph.add_weighted_edges_from) function to populate the graph with the weighted edgelist from step 1, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3. **Node Attributes:**\n",
    ">    - For each node, add attributes for the author's _display name_, _country_, _citation count_, and the _year of their first publication_ in Computational Social Science. The _display name_ and _country_ can be retrieved from your _authors_ dataset. The _year of their first publication_ and the _citation count_  can be retrieved from the _papers_ dataset.\n",
    ">    - Save the network as a JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Part 2: Preliminary Network Analysis**\n",
    "> Now, with the network constructed, perform a basic analysis to explore its features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. **Network Metrics:**\n",
    ">    - What is the total number of nodes (authors) and links (collaborations) in the network? \n",
    ">    - Calculate the network's density (the ratio of actual links to the maximum possible number of links). Would you say that the network is sparse? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">    - Is the network fully connected (i.e., is there a direct or indirect path between every pair of nodes within the network), or is it disconnected?\n",
    ">    - If the network is disconnected, how many connected components does it have? A connected component is defined as a subset of nodes within the network where a path exists between any pair of nodes in that subset. \n",
    ">    - How many isolated nodes are there in your network?  An isolated node is defined as a node with no connections to any other node in the network.\n",
    ">    - Discuss the results above on network density, and connectivity. Are your findings in line with what you expected? Why?  __(answer in max 150 words)__\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3. **Degree Analysis:**\n",
    ">    - Compute the average, median, mode, minimum, and maximum degree of the nodes. Perform the same analysis for node strength (weighted degree). What do these metrics tell us "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 4. **Top Authors:**\n",
    ">    - Identify the top 5 authors by degree. What role do these node play in the network? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">    - Research these authors online. What areas do they specialize in? Do you think that their work aligns with the themes of Computational Social Science? If not, what could be possible reasons? __(answer in max 150 words)__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
